{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91fc77a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:36: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\graha\\AppData\\Local\\Temp\\ipykernel_29608\\509306306.py:36: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  os.chdir(\"C:\\ASNA_Case_Comp\")\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math as m\n",
    "import os\n",
    "\n",
    "\n",
    "#Feel free to delete these if you don't want to install them all right away\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "#Used sm and smf instead to get these, but can still use them if needed, but sm has better summary() function\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, TweedieRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import poisson, binom, norm, lognorm\n",
    "from scipy.stats import chisquare\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score, make_scorer, roc_auc_score, log_loss, precision_score, recall_score, roc_curve, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from collections import Counter\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "\n",
    "#Change this to whereever you save your stuff\n",
    "os.chdir(\"C:\\ASNA_Case_Comp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec14a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819\n"
     ]
    }
   ],
   "source": [
    "#Read in dataset\n",
    "dat = pd.read_excel(\"Dataset.xlsx\")\n",
    "dat_one_hot = pd.read_csv(\"cleanData.csv\",header=0)\n",
    "\n",
    "#Add a column for has claim\n",
    "dat['has_claim'] = dat['claim_id'].apply(lambda x: 1 if x !=0 else 0)\n",
    "dat_one_hot['has_claim'] = dat_one_hot['claim_id'].apply(lambda x: 1 if x !=0 else 0)\n",
    "\n",
    "count = 0\n",
    "for row in dat_one_hot['has_claim']:\n",
    "    if row > 0:\n",
    "        count+=1\n",
    "print(count)\n",
    "#There is this many total claims out of 10000 students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0999800",
   "metadata": {},
   "source": [
    "# Reading in Aidan's data segmented by coverage type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea96e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\O'\n",
      "C:\\Users\\graha\\AppData\\Local\\Temp\\ipykernel_29608\\766309216.py:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  dat_expense = pd.read_csv(\"CleanByCoverage\\CleanByCoverage\\expenseClean.csv\",header=0)\n",
      "C:\\Users\\graha\\AppData\\Local\\Temp\\ipykernel_29608\\766309216.py:4: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  dat_liability = pd.read_csv(\"CleanByCoverage\\CleanByCoverage\\liabilityClean.csv\",header=0)\n",
      "C:\\Users\\graha\\AppData\\Local\\Temp\\ipykernel_29608\\766309216.py:7: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  dat_medical = pd.read_csv(\"CleanByCoverage\\CleanByCoverage\\medicalClean.csv\",header=0)\n",
      "C:\\Users\\graha\\AppData\\Local\\Temp\\ipykernel_29608\\766309216.py:10: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  dat_property = pd.read_csv(\"CleanByCoverage\\CleanByCoverage\\propertyClean.csv\",header=0)\n",
      "C:\\Users\\graha\\AppData\\Local\\Temp\\ipykernel_29608\\766309216.py:13: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  dat_expense_original = pd.read_csv(\"OriginalByCoverage\\OriginalByCoverage\\expense.csv\",header=0)\n",
      "C:\\Users\\graha\\AppData\\Local\\Temp\\ipykernel_29608\\766309216.py:16: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  dat_liability_original = pd.read_csv(\"OriginalByCoverage\\OriginalByCoverage\\liability.csv\",header=0)\n",
      "C:\\Users\\graha\\AppData\\Local\\Temp\\ipykernel_29608\\766309216.py:19: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  dat_medical_original = pd.read_csv(\"OriginalByCoverage\\OriginalByCoverage\\medical.csv\",header=0)\n",
      "C:\\Users\\graha\\AppData\\Local\\Temp\\ipykernel_29608\\766309216.py:22: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  dat_property_original = pd.read_csv(\"OriginalByCoverage\\OriginalByCoverage\\property.csv\",header=0)\n"
     ]
    }
   ],
   "source": [
    "dat_expense = pd.read_csv(\"CleanByCoverage\\CleanByCoverage\\expenseClean.csv\",header=0)\n",
    "dat_expense['has_claim'] = dat_expense['claim_id'].apply(lambda x: 1 if x !=0 else 0)\n",
    "\n",
    "dat_liability = pd.read_csv(\"CleanByCoverage\\CleanByCoverage\\liabilityClean.csv\",header=0)\n",
    "dat_liability['has_claim'] = dat_liability['claim_id'].apply(lambda x: 1 if x !=0 else 0)\n",
    "\n",
    "dat_medical = pd.read_csv(\"CleanByCoverage\\CleanByCoverage\\medicalClean.csv\",header=0)\n",
    "dat_medical['has_claim'] = dat_medical['claim_id'].apply(lambda x: 1 if x !=0 else 0)\n",
    "\n",
    "dat_property = pd.read_csv(\"CleanByCoverage\\CleanByCoverage\\propertyClean.csv\",header=0)\n",
    "dat_property['has_claim'] = dat_property['claim_id'].apply(lambda x: 1 if x !=0 else 0)\n",
    "\n",
    "dat_expense_original = pd.read_csv(\"OriginalByCoverage\\OriginalByCoverage\\expense.csv\",header=0)\n",
    "dat_expense_original['has_claim'] = dat_expense_original['claim_id'].apply(lambda x: 1 if x !=0 else 0)\n",
    "\n",
    "dat_liability_original = pd.read_csv(\"OriginalByCoverage\\OriginalByCoverage\\liability.csv\",header=0)\n",
    "dat_liability_original['has_claim'] = dat_liability_original['claim_id'].apply(lambda x: 1 if x !=0 else 0)\n",
    "\n",
    "dat_medical_original = pd.read_csv(\"OriginalByCoverage\\OriginalByCoverage\\medical.csv\",header=0)\n",
    "dat_medical_original['has_claim'] = dat_medical_original['claim_id'].apply(lambda x: 1 if x !=0 else 0)\n",
    "\n",
    "dat_property_original = pd.read_csv(\"OriginalByCoverage\\OriginalByCoverage\\property.csv\",header=0)\n",
    "dat_property_original['has_claim'] = dat_property_original['claim_id'].apply(lambda x: 1 if x !=0 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e0c65b",
   "metadata": {},
   "source": [
    "# Calculating Claim Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c434630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency for Additional Expense Coverage\n",
      "(0, 2): 2100\n",
      "(0, 3): 3124\n",
      "(0, 1): 4264\n",
      "(1, 2): 111\n",
      "(1, 1): 255\n",
      "(1, 3): 162\n",
      "Frequency for Liability Coverage\n",
      "(0, 2): 2190\n",
      "(0, 3): 3251\n",
      "(0, 1): 4474\n",
      "(1, 3): 28\n",
      "(1, 2): 19\n",
      "(1, 1): 38\n",
      "Frequency for Medical Coverage\n",
      "(0, 2): 2154\n",
      "(0, 3): 3209\n",
      "(0, 1): 4408\n",
      "(1, 3): 72\n",
      "(1, 2): 55\n",
      "(1, 1): 106\n",
      "Frequency for Property Coverage\n",
      "(0, 2): 1999\n",
      "(0, 3): 3011\n",
      "(0, 1): 4068\n",
      "(1, 1): 470\n",
      "(1, 3): 281\n",
      "(1, 2): 222\n"
     ]
    }
   ],
   "source": [
    "print('Frequency for Additional Expense Coverage')\n",
    "claim_frequency_expense = list(zip(dat_expense_original['has_claim'], dat_expense_original['risk_tier']))\n",
    "claim_frequency_expense_counts = Counter(claim_frequency_expense)\n",
    "for item, count in claim_frequency_expense_counts.items():\n",
    "    print(f'{item}: {count}')\n",
    "\n",
    "print('Frequency for Liability Coverage')\n",
    "claim_frequency_liability = list(zip(dat_liability_original['has_claim'], dat_liability_original['risk_tier']))\n",
    "claim_frequency_liability_counts = Counter(claim_frequency_liability)\n",
    "for item, count in claim_frequency_liability_counts.items():\n",
    "    print(f'{item}: {count}')\n",
    "\n",
    "print('Frequency for Medical Coverage')\n",
    "claim_frequency_medical = list(zip(dat_medical_original['has_claim'], dat_medical_original['risk_tier']))\n",
    "claim_frequency_medical_counts = Counter(claim_frequency_medical)\n",
    "for item, count in claim_frequency_medical_counts.items():\n",
    "    print(f'{item}: {count}')\n",
    "\n",
    "print('Frequency for Property Coverage')\n",
    "claim_frequency_property = list(zip(dat_property_original['has_claim'], dat_property_original['risk_tier']))\n",
    "claim_frequency_property_counts = Counter(claim_frequency_property)\n",
    "for item, count in claim_frequency_property_counts.items():\n",
    "    print(f'{item}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa682bc",
   "metadata": {},
   "source": [
    "# Modelling Claim Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d0cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Expense Loss: 2,911,317.09\n",
      "Total Expense Claims: 528\n",
      "Expense Severity (Loss per Claim): 5,513.86\n",
      "Total Property Loss: 995,460.03\n",
      "Total Property Claims: 973\n",
      "Property Severity (Loss per Claim): 1,023.08\n",
      "Total Medical Loss: 3,870,552.10\n",
      "Total Medical Claims: 233\n",
      "Medical Severity (Loss per Claim): 16,611.81\n",
      "Total Liability Loss: 737,410.81\n",
      "Total Liability Claims: 85\n",
      "Liability Severity (Loss per Claim): 8,675.42\n"
     ]
    }
   ],
   "source": [
    "total_loss_expense = dat_expense['amount'].sum()\n",
    "print(f\"Total Expense Loss: {total_loss_expense:,.2f}\")\n",
    "\n",
    "total_claims_expense = dat_expense['has_claim'].sum()\n",
    "print(f\"Total Expense Claims: {total_claims_expense}\")\n",
    "\n",
    "severity_expense = total_loss_expense / total_claims_expense\n",
    "print(f\"Expense Severity (Loss per Claim): {severity_expense:,.2f}\")\n",
    "\n",
    "total_loss_property = dat_property['amount'].sum()\n",
    "print(f\"Total Property Loss: {total_loss_property:,.2f}\")\n",
    "\n",
    "total_claims_property = dat_property['has_claim'].sum()\n",
    "print(f\"Total Property Claims: {total_claims_property}\")\n",
    "\n",
    "severity_property = total_loss_property / total_claims_property\n",
    "print(f\"Property Severity (Loss per Claim): {severity_property:,.2f}\")\n",
    "\n",
    "total_loss_medical = dat_medical['amount'].sum()\n",
    "print(f\"Total Medical Loss: {total_loss_medical:,.2f}\")\n",
    "\n",
    "total_claims_medical = dat_medical['has_claim'].sum()\n",
    "print(f\"Total Medical Claims: {total_claims_medical}\")\n",
    "\n",
    "severity_medical = total_loss_medical / total_claims_medical\n",
    "print(f\"Medical Severity (Loss per Claim): {severity_medical:,.2f}\")\n",
    "\n",
    "total_loss_liability = dat_liability['amount'].sum()\n",
    "print(f\"Total Liability Loss: {total_loss_liability:,.2f}\")\n",
    "\n",
    "total_claims_liability = dat_liability['has_claim'].sum()\n",
    "print(f\"Total Liability Claims: {total_claims_liability}\")\n",
    "\n",
    "severity_liability = total_loss_liability / total_claims_liability\n",
    "print(f\"Liability Severity (Loss per Claim): {severity_liability:,.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08cd5bd",
   "metadata": {},
   "source": [
    "# Stress Test: 1000 more people in greek residences ~ probability rise by 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f2f365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\graha\\AppData\\Local\\Temp\\ipykernel_29608\\2657103005.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  capital_efficiency = EL / PML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Expected_Loss</th>\n",
       "      <th>PML_99</th>\n",
       "      <th>Capital_Efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Expense</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>6336.773687</td>\n",
       "      <td>366.961428</td>\n",
       "      <td>9708.745316</td>\n",
       "      <td>0.037829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Property</td>\n",
       "      <td>0.105385</td>\n",
       "      <td>1145.821943</td>\n",
       "      <td>120.751050</td>\n",
       "      <td>2380.204029</td>\n",
       "      <td>0.050769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medical</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>20132.670850</td>\n",
       "      <td>502.023272</td>\n",
       "      <td>14323.787789</td>\n",
       "      <td>0.035181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liability</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>10617.385487</td>\n",
       "      <td>102.248532</td>\n",
       "      <td>340.162902</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Coverage  Frequency      Severity  Expected_Loss        PML_99  \\\n",
       "0    Expense   0.057910   6336.773687     366.961428   9708.745316   \n",
       "1   Property   0.105385   1145.821943     120.751050   2380.204029   \n",
       "2    Medical   0.024934  20132.670850     502.023272  14323.787789   \n",
       "3  Liability   0.009633  10617.385487     102.248532    340.162902   \n",
       "\n",
       "   Capital_Efficiency  \n",
       "0            0.037829  \n",
       "1            0.050769  \n",
       "2            0.035181  \n",
       "3                 inf  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have previously shown that likelihood that person has sprinkler follows a binomial p=0.7\n",
    "#We have also shown claims accross all types of insurance follow some sort of lognormal\n",
    "\n",
    "#Steps (Done using a Monte Carlo Simulation)\n",
    "#I have calculated all the means, variances and probabilities in Case_Comp_Modelling, simply saving them here for easy use\n",
    "\n",
    "#Step 1:\n",
    "#Simulate 20% increase in people off-campus (Binomial Random Sample ~p=0.16 regardless of types)\n",
    "greek_binomial_p = 0.197\n",
    "#We can add 0.1 because the dataset is 10000 people, and I want to simulate a 1000 person increase\n",
    "greek_binomial_p_new = greek_binomial_p + 0.1\n",
    "\n",
    "\n",
    "#Step 2:\n",
    "#Simulate whether the person has a claim or not (Binomial Random Sample, different P for each coverage type)\n",
    "\n",
    "#[greek probability of claim, no greek probability of claim]\n",
    "coverage_binomial_p = {\"Expense\": [0.0947,0.0424],\n",
    "                       \"Property\": [0.1669,0.0795],\n",
    "                       \"Medical\": [0.0361,0.0202],\n",
    "                       \"Liability\": [0.0178,0.0062]}\n",
    "\n",
    "#Step 3:\n",
    "#Simulate Claim Amounts (Different Lognormal for each coverage type) [mu,std]\n",
    "\n",
    "#[greek [mu,sigma], no greek [mu,sigma]]\n",
    "coverage_lognormal_params = {\"Expense\": [[8.792251500587787,0.7860062738715917],[8.030335656316481,0.6733062976959585]],\n",
    "                             \"Property\": [[7.069795665992642,0.7414021940612359],[6.42011980277624,0.7083362835890147]],\n",
    "                             \"Medical\": [[9.75170860304146,1.0449930588238456],[8.979976322517073,0.9893257823172484]],\n",
    "                             \"Liability\": [[8.873775404507777,1.067439697992325],[8.425258924450155,1.0886178366432073]]}\n",
    "\n",
    "#Step 4: Calculate final Values for Frequency, Severity, Expected Loss(E[Loss]) and Probable Maximum Loss(PML)\n",
    "\n",
    "np.random.seed(123)\n",
    "#Here is the code to do the simulation\n",
    "N_people = 10000\n",
    "N_simulations = 5000\n",
    "\n",
    "# Store results for all four coverage types\n",
    "results = {cov: {\"frequency\": [], \"severity\": [], \"EL\": [], \"PML\": [], \"capital_efficiency\": []}for cov in coverage_binomial_p.keys()}\n",
    "\n",
    "#Iterate through 5000 simulations\n",
    "for _ in range(N_simulations):\n",
    "\n",
    "    # Step 1: simulate binomial for increased off-campus presence\n",
    "    offcampus = np.random.binomial(1, greek_binomial_p_new, N_people)\n",
    "\n",
    "    #Iterating over the four types of claims\n",
    "    for cov in coverage_binomial_p:\n",
    "\n",
    "        #Fetch the probabilities for off and on campus, recall they are stored as [probability given off campus, probability given on campus]\n",
    "        p_off, p_on = coverage_binomial_p[cov]\n",
    "        (mu_off, sig_off), (mu_on, sig_on) = coverage_lognormal_params[cov]\n",
    "\n",
    "        #Get indices where claims are off and on campus\n",
    "        off_idx = np.where(offcampus == 1)[0]\n",
    "        on_idx  = np.where(offcampus == 0)[0]\n",
    "\n",
    "        #Calculate whether they have a claim based on whether they are off or on campus\n",
    "        has_claim = np.zeros(N_people, dtype=int)\n",
    "        has_claim[off_idx] = np.random.binomial(1, p_off, size=len(off_idx))\n",
    "        has_claim[on_idx]  = np.random.binomial(1, p_on,  size=len(on_idx))\n",
    "\n",
    "        #Calculate severity for off and on campus using the lognorm\n",
    "        severity_vals = np.zeros(N_people)\n",
    "\n",
    "        #Off-campus random value generation from off capmus lognorm\n",
    "        off_claim_idx = off_idx[has_claim[off_idx] == 1]\n",
    "        if len(off_claim_idx) > 0:\n",
    "            severity_vals[off_claim_idx] = lognorm(s=sig_off, scale=np.exp(mu_off)).rvs(len(off_claim_idx))\n",
    "\n",
    "        #On-campus random value generation from on campus lognorm\n",
    "        on_claim_idx = on_idx[has_claim[on_idx] == 1]\n",
    "        if len(on_claim_idx) > 0:\n",
    "            severity_vals[on_claim_idx] = lognorm(s=sig_on, scale=np.exp(mu_on)).rvs(len(on_claim_idx))\n",
    "\n",
    "        #Calculate The metrics from these values\n",
    "        freq = has_claim.mean()\n",
    "\n",
    "        sev = severity_vals[severity_vals > 0].mean() if has_claim.sum() > 0 else 0\n",
    "\n",
    "        EL = severity_vals.sum() / N_people\n",
    "\n",
    "        PML = np.percentile(severity_vals, 99)\n",
    "\n",
    "        capital_efficiency = EL / PML\n",
    "\n",
    "        #Store them in results dictionary\n",
    "        results[cov][\"frequency\"].append(freq)\n",
    "        results[cov][\"severity\"].append(sev)\n",
    "        results[cov][\"EL\"].append(EL)\n",
    "        results[cov][\"PML\"].append(PML)\n",
    "        results[cov][\"capital_efficiency\"].append(capital_efficiency)\n",
    "\n",
    "# Final summary\n",
    "summary = []\n",
    "for cov in results:\n",
    "    summary.append({\n",
    "        \"Coverage\": cov,\n",
    "        \"Frequency\": np.mean(results[cov][\"frequency\"]),\n",
    "        \"Severity\": np.mean(results[cov][\"severity\"]),\n",
    "        \"Expected_Loss\": np.mean(results[cov][\"EL\"]),\n",
    "        \"PML_99\": np.mean(results[cov][\"PML\"]),\n",
    "        \"Capital_Efficiency\": np.mean(results[cov][\"capital_efficiency\"])\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f40279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(dat_input, coverage, N_simulations=50_000, pml_quantile=0.99):\n",
    "\n",
    "    # --- Clean severity data ---\n",
    "    severity_data = dat_input['amount']\n",
    "    severity_data = severity_data.replace([np.inf, -np.inf], np.nan)\n",
    "    severity_data = severity_data[severity_data > 0].dropna()\n",
    "\n",
    "    # --- Fit lognormal ---\n",
    "    sigma, loc, scale = st.lognorm.fit(severity_data, floc=0)\n",
    "    mu = np.log(scale)\n",
    "\n",
    "    # --- Frequency ---\n",
    "    exposures = dat_input[['student_id']].drop_duplicates()\n",
    "    frequency = dat_input['has_claim'].sum() / len(exposures)\n",
    "\n",
    "    # --- Severity (lognormal mean) ---\n",
    "    severity = np.exp(mu + 0.5 * sigma**2)\n",
    "\n",
    "    # --- Expected Loss per Person ---\n",
    "    expected_loss = frequency * severity\n",
    "\n",
    "    # --- Monte Carlo for *per-person* loss ---\n",
    "    # 1. Claim indicator for each simulation: 0 or 1\n",
    "    claim_indicator = np.random.binomial(1, frequency, size=N_simulations)\n",
    "\n",
    "    # 2. A severity draw for each simulation\n",
    "    severity_draws = np.random.lognormal(mean=mu, sigma=sigma, size=N_simulations)\n",
    "\n",
    "    # 3. Per-person simulated losses\n",
    "    losses = claim_indicator * severity_draws\n",
    "\n",
    "    # --- Per-Person PML ---\n",
    "    pml = np.percentile(losses, pml_quantile * 100)\n",
    "\n",
    "    # Package results\n",
    "    results = {\n",
    "        \"Coverage\": coverage,\n",
    "        \"Frequency\": frequency,\n",
    "        \"Severity\": severity,\n",
    "        \"Expected_Loss\": expected_loss,\n",
    "        \"PML_99\": pml\n",
    "    }\n",
    "\n",
    "    results_df = pd.DataFrame([results])\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b51590b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Expected_Loss</th>\n",
       "      <th>PML_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Expense</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>5554.517874</td>\n",
       "      <td>293.278544</td>\n",
       "      <td>8064.856122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Property</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>1041.362050</td>\n",
       "      <td>101.324528</td>\n",
       "      <td>2038.790459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medical</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>17762.891693</td>\n",
       "      <td>413.875376</td>\n",
       "      <td>11793.021351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liability</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>10071.334655</td>\n",
       "      <td>85.606345</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Coverage  Frequency      Severity  Expected_Loss        PML_99\n",
       "0    Expense     0.0528   5554.517874     293.278544   8064.856122\n",
       "1   Property     0.0973   1041.362050     101.324528   2038.790459\n",
       "2    Medical     0.0233  17762.891693     413.875376  11793.021351\n",
       "3  Liability     0.0085  10071.334655      85.606345      0.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "results_expense_df = calculate_stats(dat_expense, 'Expense')\n",
    "results_property_df = calculate_stats(dat_property, 'Property')\n",
    "results_medical_df = calculate_stats(dat_medical, 'Medical')\n",
    "results_liability_df = calculate_stats(dat_liability, 'Liability')\n",
    "\n",
    "df_all_results = pd.concat([results_expense_df, results_property_df, results_medical_df, results_liability_df], ignore_index=True)\n",
    "df_all_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
